import json, re
from pathlib import Path
from collections import defaultdict
from typing import Dict, Set, List, Tuple

# ‚îÄ‚îÄ Í≤ΩÎ°ú ÏÑ§Ï†ï ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
RESULTS_PATH = Path("Result/Ours/hotpot_result_sampled_v5.json")   # Î¶¨Ìä∏Î¶¨Î≤Ñ Í≤∞Í≥º (200Í∞ú)
GOLD_PATH    = Path("Result/hotpot_gold_chunk_ids.json")           # gold 1‚ÄØ000Í∞ú
STORE_PATH   = Path("hotpotQA/hotpot_kv_store_text_chunks.json")   # Ï†ÑÏ≤¥ chunk store
OUT_PATH     = Path("Result/Ours/retriever_eval.json")             # ÌèâÍ∞Ä ÏßÄÌëú Ï†ÄÏû•

# ‚îÄ‚îÄ 1) chunk store Î°úÎìú ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
print("üìÇ loading chunk store ...")
with STORE_PATH.open(encoding="utf-8") as f:
    STORE: Dict[str, str] = json.load(f)        # {chunk_id: {"content": "...", ...}, ...}
CONTENT_MAP = {cid: obj["content"] for cid, obj in STORE.items()}

# (ÏÑ±Îä• up) ÏÑ†Îëê 60Ïûê ‚Üí id Ïù∏Îç±Ïä§
HEAD2ID = defaultdict(list)
for cid, text in CONTENT_MAP.items():
    head = text.strip()[:60]          # Ïïû 60Ïûê
    HEAD2ID[head].append(cid)

# ‚îÄ‚îÄ 2) Î¶¨Ìä∏Î¶¨Î≤Ñ Í≤∞Í≥º Î°úÎìú & chunk‚Äëid Îß§Ìïë ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
CHUNK_SPLIT = re.compile(r"\[Chunk\s+\d+\]")

def text2id(chunk_text: str) -> str | None:
    """
    chunk Î≥∏Î¨∏ÏùÑ Î∞õÏïÑ storeÏóêÏÑú Í∞ôÏùÄ ÎÇ¥Ïö©ÏùÑ Í∞ÄÏßÑ id Î∞òÌôò.
    - Ï†ïÌôïÌûà Í∞ôÏßÄ ÏïäÏïÑÎèÑ Ï∂©Î∂ÑÌûà Í∏¥ substring Îß§Ïπ≠Ïù¥Î©¥ OK
    """
    snippet = chunk_text.strip()[:60]
    # 1) Îπ†Î•∏ Í≤ΩÎ°ú: ÏÑ†Îëê 60Ïûê ÏôÑÏ†Ñ ÏùºÏπò
    for cid in HEAD2ID.get(snippet, []):
        return cid
    # 2) fallback: Ï†ÑÏ≤¥ store scanning (ÎäêÎ¶¨ÏßÄÎßå 200√ótopk Ï†ïÎèÑÎ©¥ Í∞êÎãπ Í∞ÄÎä•)
    for cid, text in CONTENT_MAP.items():
        if snippet and snippet in text:
            return cid
    return None

def parse_retrieved_chunks(context_token: str) -> Set[str]:
    """
    context_token Î¨∏ÏûêÏó¥ -> {chunk_hash_id ...}
    """
    if not isinstance(context_token, str):
        return set()

    parts = CHUNK_SPLIT.split(context_token)    # ['', 'Î≥∏Î¨∏1', 'Î≥∏Î¨∏2', ...]
    # parts[0]Îäî [Chunk] Ïù¥Ï†Ñ Í≥µÎ∞±Ïù¥ÎØÄÎ°ú Î≤ÑÎ¶¨Í≥† ÎÇòÎ®∏ÏßÄ Î≥∏Î¨∏Îì§Îßå
    ids = set()
    for chunk_body in parts[1:]:
        cid = text2id(chunk_body)
        if cid:
            ids.add(cid)
    return ids

print("üìÇ loading retriever output ...")
with RESULTS_PATH.open(encoding="utf-8") as f:
    data = json.load(f)    # 200Í∞ú Ìï≠Î™© Î∞∞Ïó¥

retrieved: Dict[str, Set[str]] = defaultdict(set)
for obj in data:
    q = obj["query"]
    ctx = obj.get("context_token", "")
    retrieved[q].update(parse_retrieved_chunks(ctx))

# ‚îÄ‚îÄ 3) gold Î°úÎìú ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
with GOLD_PATH.open(encoding="utf-8") as f:
    gold_items = json.load(f)
gold = {item["query"]: set(item["gold_chunk_ids"]) for item in gold_items}

# ‚îÄ‚îÄ 4) ÌèâÍ∞Ä ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def prf(tp: int, fp: int, fn: int) -> Tuple[float, float, float]:
    p = tp / (tp + fp) if tp + fp else 0.0
    r = tp / (tp + fn) if tp + fn else 0.0
    f = 2 * p * r / (p + r) if p + r else 0.0
    return p, r, f

eval_queries = [q for q in retrieved if q in gold]   # 200Í∞ú ÍµêÏßëÌï©
per_query, S_tp, S_fp, S_fn = [], 0, 0, 0

for q in eval_queries:
    g = gold[q]
    r = retrieved[q]
    tp = len(r & g)
    fp = len(r - g)
    fn = len(g - r)
    p, rcl, f1 = prf(tp, fp, fn)
    per_query.append({"query": q, "tp": tp, "fp": fp, "fn": fn,
                      "precision": p, "recall": rcl, "f1": f1})
    S_tp += tp; S_fp += fp; S_fn += fn

micro_p, micro_r, micro_f1 = prf(S_tp, S_fp, S_fn)
macro_p = sum(d["precision"] for d in per_query) / len(per_query)
macro_r = sum(d["recall"]    for d in per_query) / len(per_query)
macro_f1 = sum(d["f1"]       for d in per_query) / len(per_query)

summary = {
    "n_queries": len(per_query),
    "micro": {"precision": micro_p, "recall": micro_r, "f1": micro_f1},
    "macro": {"precision": macro_p, "recall": macro_r, "f1": macro_f1},
}

OUT_PATH.parent.mkdir(parents=True, exist_ok=True)
with OUT_PATH.open("w", encoding="utf-8") as f:
    json.dump({"summary": summary, "per_query": per_query},
              f, ensure_ascii=False, indent=2)

print("‚úÖ ÌèâÍ∞Ä ÏôÑÎ£å ‚Äî", summary["n_queries"], "Í∞ú ÏøºÎ¶¨")
print("‚Ä¢ Micro  P/R/F1 : "
      f"{micro_p:.3f} / {micro_r:.3f} / {micro_f1:.3f}")
print("‚Ä¢ Macro  P/R/F1 : "
      f"{macro_p:.3f} / {macro_r:.3f} / {macro_f1:.3f}")
print("‚Üí ÏÑ∏Î∂Ä Í≤∞Í≥º:", OUT_PATH)
